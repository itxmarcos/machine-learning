{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc1fa6d8",
   "metadata": {},
   "source": [
    "# Using Scrapy in Jupyter Notebook\n",
    "This notebook makes use of the Scrapy library to scrape data from a website. Following the basic example, we create a QuotesSpider and call the CrawlerProcess with this spider to retrieve quotes from http://quotes.toscrape.com.\n",
    "\n",
    "In this notebook two pipelines are defined, both writing results to a JSON file. The first option is to create a separate class that defines the pipeline and explicitly has the functions to write to a file per found item. It enables more flexibility when dealing with stranger data formats, or if you want to setup a custom way of writing items to file. The pipeline is set in the custom_settings parameter ITEM_PIPELINES inside the QuoteSpider class. However, I simply want to write the list of items that are found in the spider to a JSON file and therefor it is easier to choose the second option, where only the FEED_FORMAT has to be set to JSON and the output file needs to be defined in FEED_URI inside the custom settings of the spider. No additional classes or definitions need to be created, making the FEED_FORMAT/FEED_URI a convenient option.\n",
    "\n",
    "Once the quotes are retrieved the JSON file will be created on disk and can be loaded to a Pandas dataframe. This dataframe can then be analyzed, modified and be used for further processing. This notebook simply loads the JSON file to a dataframe and writes it again to a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b17fc40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Show Python version\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e6f3e",
   "metadata": {},
   "source": [
    "## Import Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036c643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import scrapy\n",
    "except:\n",
    "    !pip install scrapy\n",
    "    import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1614de8",
   "metadata": {},
   "source": [
    "## Setup a pipeline\n",
    "This class creates a simple pipeline that writes all found items to a JSON file, where each line contains one JSON element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7a333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('quoteresult.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9fc6ec",
   "metadata": {},
   "source": [
    "## Define the spider\n",
    "The QuotesSpider class defines from which URLs to start crawling and which values to retrieve. I set the logging level of the crawler to warning, otherwise the notebook is overloaded with DEBUG messages about the retrieved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2189b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
    "        'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
    "        'FEED_URI': 'quoteresult.json'                        # Used for pipeline 2\n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'author': quote.css('span small::text').extract_first(),\n",
    "                'tags': quote.css('div.tags a.tag::text').extract(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640b21a",
   "metadata": {},
   "source": [
    "## Start the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5afa3188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 13:02:22 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-01 13:02:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 22.0.0 (OpenSSL 3.0.3 3 May 2022), cryptography 37.0.2, Platform Windows-10-10.0.19043-SP0\n",
      "2022-06-01 13:02:22 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2022-06-01 13:02:23 [py.warnings] WARNING: C:\\Users\\mcaballero\\Miniconda3\\envs\\wine\\lib\\site-packages\\scrapy\\extensions\\feedexport.py:289: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x2c46b22f970>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(QuotesSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424a326",
   "metadata": {},
   "source": [
    "## Check the files\n",
    "Verify that the files has been created on disk. As we can observe the files are both created and have data. The .jl file has line separated JSON elements, while the .json file has one big JSON array containing all the quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6e0749",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1179952458.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [6]\u001b[1;36m\u001b[0m\n\u001b[1;33m    ll quoteresult.*\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ll quoteresult.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a77873",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 2 quoteresult.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71666c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 2 quoteresult.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72103940",
   "metadata": {},
   "source": [
    "## Create dataframes\n",
    "Pandas can now be used to create dataframes and save the frames to pickles. The .sjon file can be loaded directly into a frame, whereas for the .jl file we need to specify the JSON objects are divided per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c2041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfjson = pd.read_json('quoteresult.json')\n",
    "dfjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfjl = pd.read_json('quoteresult.jl', lines=True)\n",
    "dfjl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83de1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfjson.to_pickle('quotejson.pickle')\n",
    "dfjl.to_pickle('quotejl.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec823c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll *pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9abb4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0521eec06ea9fd8803d3ee995650ee37817d66a08ccd6fc1f3259788c384cbda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
